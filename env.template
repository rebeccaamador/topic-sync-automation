# Topic Sync Automation - Environment Variables Template
# 
# INSTRUCTIONS:
# 1. Copy this file: cp env.template .env
# 2. Edit .env with your actual values
# 3. Load variables: source .env
# 4. Run automation: python3 scripts/parallel_pr_creator.py
#
# Note: .env is gitignored for security (contains tokens)

# =============================================================================
# REQUIRED VARIABLES
# =============================================================================

# Kafka topic to add (e.g., audit.action.v1, device.state.v2)
export TOPIC="audit.action.v1"

# GitHub Personal Access Token (requires repo write permissions)
# Create at: https://github.com/settings/tokens
export GITHUB_TOKEN="ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# Slack webhook URL for notifications
# Get from: https://api.slack.com/messaging/webhooks
# Test it: cd local-testing && ./test-slack-notification.sh
export SLACK_WEBHOOK="https://hooks.slack.com/services/XXXXXXXXX/XXXXXXXXX/XXXXXXXXXXXXXXXXXXXXXXXX"

# Repository names (format: org/repo)
export HELM_APPS_REPO="your-org/helm-apps"
export DATA_AIRFLOW_REPO="your-org/data-airflow"
export DBT_REPO="your-org/dbt"

# =============================================================================
# OPTIONAL VARIABLES (with defaults)
# =============================================================================

# Value type: json or protobuf
export VALUE_TYPE="json"

# Sink type: realtime (Snowflake) or s3
export SINK_TYPE="realtime"

# Default branches for each repository
export HELM_BRANCH="master"
export AIRFLOW_BRANCH="develop"
export DBT_BRANCH="master"

# =============================================================================
# AIRFLOW DAG INFO (optional - for Slack manual trigger link, realtime sinks)
# =============================================================================

# Airflow base URL (e.g., https://airflow.company.com)
# If set, Slack notification will include a clickable link to manually trigger the DAG
export AIRFLOW_URL="https://airflow.company.com"

# Airflow DAG ID for manual trigger link
export AIRFLOW_DAG_ID="kafka_stream_loader"

# Note: No credentials needed! The Slack notification will provide a link
# for you to manually trigger the DAG after merging PRs.

# =============================================================================
# SNOWFLAKE VARIABLES (optional - enables schema auto-discovery for dbt)
# =============================================================================

# Snowflake account identifier (e.g., abc12345.us-east-1)
export SNOWFLAKE_ACCOUNT="your-account.region"

# Snowflake username (your email)
export SNOWFLAKE_USER="your.email@company.com"

# Snowflake password (optional if using SSO via externalbrowser)
# export SNOWFLAKE_PASSWORD="your-password"

# Authentication method (use "externalbrowser" for SSO)
export SNOWFLAKE_AUTHENTICATOR="externalbrowser"

# Snowflake warehouse
export SNOWFLAKE_WAREHOUSE="LOADER_PRODUCTION_STREAMING"

# Snowflake database
export SNOWFLAKE_DATABASE="PRODUCTION"

# Snowflake schema
export SNOWFLAKE_SCHEMA="public"

# Snowflake role (optional)
export SNOWFLAKE_ROLE="ANALYST"

# =============================================================================
# QUICK START EXAMPLES
# =============================================================================
#
# Example 1: Realtime (Snowflake) Sink
# -------------------------------------
# export TOPIC="audit.action.v1"
# export VALUE_TYPE="json"
# export SINK_TYPE="realtime"
# python3 scripts/parallel_pr_creator.py
#
# This creates:
# - helm-apps PR: Adds topic to Snowflake sink connector
# - data-airflow PR: Adds StreamTaskConfig for Airflow processing
# - dbt PR: Creates extraction models (stg_kafka__audit__action__v1__extracted.sql)
#
# Example 2: S3 Sink
# -------------------
# export TOPIC="analytics.event.v1"
# export VALUE_TYPE="protobuf"
# export SINK_TYPE="s3"
# python3 scripts/parallel_pr_creator.py
#
# This creates:
# - helm-apps PR: Adds topic to S3 sink connector
# - data-airflow: SKIPPED (not needed for S3)
# - dbt PR: Creates external source definitions for S3 data
#
# Example 3: Local Dry-Run Testing
# ---------------------------------
# cd local-testing
# ./dry-run-step1.sh audit.action.v1 json realtime
# ./dry-run-step2.sh audit.action.v1
# ./dry-run-step3.sh audit.action.v1 realtime
#
# =============================================================================

